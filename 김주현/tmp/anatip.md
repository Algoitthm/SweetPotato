## 분석 대분류

1. 예측분석
2. 분류분석
3. 수요예측
   1. 단기적으로 이동평균 MA나 지수평활법
   2. Arima 다중선형회귀
4. 추천개인화
5. 최적화
6. 텍스트마이닝
7. 기초통계분석
8. 딥러닝
   1. (Data/언어/시각 AI)
9. 세분화
10. 품질 등 원인인자 분석
   11. CNN 불량예측
   12. Bayesian Network를 활용한 이상원인 해석 및 변수간 인과관계
13. 이상감지(설비예지보전, FDS등)
    1. 분류모델과 군집화 기법을 활용한 이상패턴 탐지
14. 군집분석(개인화)
15. 추천분석(상품추천)
16. Text Analytics
    1. word2vec
       1. skip gram 방식 : 중심단어를 사용하여 주변단어를 예측한다.
       2. CBOW 방식 : 주변 단어를 사용하여 중심단어를 예측한다.



## 머신러닝의 종류

1. 지도학습

   1. FDS(사기적발), 음성이미지인식, 이메일분류
   2. Tree계열 : 결정트리, RF
   3. network계열 : 로지스틱, 뉴럴
   4. 커널계열 : SVM

2. 비지도학습

   1. 클러스터링, 문서분류. 고객Segmentation

   2. Distance이용 : Hierarchy cluster

   3. Centroid이용 : K-means, EM알고리즘

      

## 설명력

1. 있음
   1. KNN
   2. Decision Tree
   3. Logistic regression
      1. 독립 to 종속변수 영향 직관적
      2. Odds비 산출
      3. 신용평가모형에서 스코어 산출에 사용

## 분석 순서

1. 분석 목표수립
   1. 모델링 알고리즘 파악
   2. 평가기준 수립
2. 데이터 마트구성
3. EDA해서 알고리즘 적용가능성 파악
4. 데이터 전처리 및 파생변수 도출
5. 알고리즘 적용
6. 최적 모델 선택
7. 모형의 결과 시각화



## 예시별 분석법

1. 통신사 해지방어 모델 (카드사기 적발 유사)
   1. 상세
      1. 검색이력 , 고객 데이터
      2. 1개월 이내 해지여부 예측
   2. 해결
      1. Decision Tree로 주요 해지요인 분석
      2. RF, 인공신경망으로 예측정확도 상승
2. 보험FDS (퇴직예측 유사)
   1. 상세
      1. 보험 청구건으로 사기유형 및 사기여부 예측
   2. 해결
      1. 사기 유형을 비슷한것 끼리 클러스터링
         1. PCA를 통해 5차원벡터를 분산이 최대인 2차원으로 수정
      2. 사기유형 별로 사기위험 예측모델 구현
         1. 로지스틱 회귀 사용
            1. 독립변수간 다중공산성 제거(상관관계 높은 것)
            2. 불필요 변수 제거
               1. AIC 산출로 변수를 하나씩 지워가며 모델 변별력 확인
            3. 변수간 스케일 일치
               1. 아웃라이어 제거를 위해 범주형 변수로 변환
            4. 사기건수:정상건수 유사
               1. 사기건 오버샘플링
               2. 정상건 언더샘플링
3. 텍스트분석
   1. 텍스트 전처리
      1. 불용어, 어간분석, 품사 태깅, Syntactic Parsing
      2. 변환
         1. 수치적으로 변환
      3. Feature Selection
         1. LSA LDA 나이브베이즈, SVM 등

** 홈페이지의 로그 데이터. 사용자 정보X, 최대 2주 저장 비식별 데이터, 이동순서 및 방문페이지 정보 있음. XXX 구매가능성을 수치화. 

:: 분석을 위한 변수 정의 및 데이터셋 구성

> 답안 :: id_key를 기준, 사용자별로 요약 통계량 만들면 됨. 통계량 만들시 3가지 축을 고려.(시간별/품목별/항목별)
>          1) 시간 (ex. per day/week/total/ ... )
>          2) 품목 : 분석목적에 따라 달라지나, 대분류 > 중분류 > 소분류 순으로 선정 .
>               (ex. 가전제품 전체 / 에어컨 전체 / 대체재 / 보완재 페이지)
>          3) 항목별 : count 가능한 RFM 기반 통계량 생성. 마케팅 부서와의 협의 후 고도화.
> 				*** RFM - Recency 최근성 (최근방문횟수,  방문주기) / 
>                               Frequency 빈도 (Total 방문횟수, 장바구니 담은 항목 수) / 
>                              Monetory 금액(장바구니 담은 품목의 액수)



 :: 분석 모델링 및 검증방안 수립
> 답안 : Lasso 또는 RF의 변수중요도를 이용하여 변수선정 후 Logistic Regression로 0과 1사이의 지표 생성
> (-7) 데이터/모델링 진단 과정 미흡,  검증 방안 기준 미제시, 활용 측면 검증, 개선 불충분

:: 클라우드 환경 하의 모델링 배포 방안
> 답안 : BigQuery ML 기준, Google Cloud Storage 버킷에 업로드 후 모델리소스 생성. (알아서작성)



== 2. 화학 공정에서의 불량 판정 ==

** 문제요약 : 펠릿?을 공정별 생산 데이터. 3개의 공정, 공정별로 Interval 존재. 투입공정부터 생산완료공정까지 최종적으로 걸리는 시간은 4시간 30분. 공정별로 시간(초단위) 및 Setting Value(V1-V7) 및 생산 결과(V8, V9)? 항목 테이블 있음.

:: 데이터마트 생성

> 답안 : Step 1) 초단위 데이터를 분단위 데이터로 변경하기 위한 요약통계량 생성(Min / Max / Mean / Mode)
>
> ​         Step 2)  각 공정별 시간차를 반영하여, 시간을 기준으로 결합.
> ​	     Step 3)  생산결과 (V8, V9) 항목을 보고 기준미달인 경우 1(불량), 0(정상)으로 태깅
>
> c.f_1) Pseudo Code로 작성안해도 처리방안만 단계별로 서술하면 미감점이었음
>
> c.f_2) 오창 전지공장은 공정별로 Lot ID가 있어서 시간지연처리 없이 Trace Table을 이용하여 Mapping 가능함.



:: 최적 공정운영조건 도출

> 답안 : Step 1) 7개 Setting Value들에 대한 4개의 요약통계량을 독립변수(총 7x4 = 28개), 
>                    OK_NG(0,1)를 종속변수로 하여 Random Forest 모델 적합.
>
> ​         Step 2) 중요도 순위 상위 5~10개(기준은 현업과 협의)인자로 Decion Tree 적용해보고, 분류가 되는 범위를 확인
> ​					ex) 최상위 중요도 인자로 나온 X3_min이 36보다 크면 불량이 많을때, 
> ​                         X3의 값은 36보다 작게 설정해야 함을 확인할 수 있음.
>
> ​		Step 3) 변수별 값 범위 EDA를 통해 수율이 낮은 구간검증
> ​        Step 4) 현업과 협의 후 중요인자별 최적운영조건 정리
>
> (-1) 검증방법 미흡
>  (-1) 운영조건 도출방식 미흡

:: 최적 투입량 분석

> 답안 : 공정별 Setting Value 조합인 Recipe를 정리. 수율순으로 내림차순정렬하여 가장 높은 수율일때의 Recipe를 취함.
> 감점요인 : (-2) 선형여부 미지정
>             (-2) x2-x7투입방법 미지정
>             (-2) 시뮬레이션방법론 미흡 





## 데이터 분석 클라우드 환경

1. AWS
   1. Kinesis : 실시간 스트리밍 데이터 수집, 기본은 ETL이지만 java확장하여 스트리밍데이터분석에 사용
   2. SageMaker : 머신러닝 모델 학습, 배포
   3. EC2 : 가상서버
   4. S3 : 스토리지
   5. Athena : S3에 SQL쿼리
   6. RedShift : 데이터 웨어하우스만들기
   7. QuichSight : BI툴
   8. EMR : 분산프레임워크 제공
2. DAP MLDL
   1. 원천 데이터는 SBP플랫폼에서 가져옴(Data Lake)
   2. Paxata :전처리
   3. 노트북에서 모델 개발
   4. 모델 저장
   5. 평가지표와 목표점수 입력
   6. 이 과정을 job으로 배치진행
   7. 각 결과에 대해 모니터링
3. AutoML
   1. 변수 선정 및 유관데이터 선별 필요, 다양한 접근 방식의 분석가 필요
   2. 하나의 데이터 테이블화 해주어야 함
4. PySpark
   1. Spark SQL : SQL쿼리로 데이터처리 용이
   2. MLlib : 머신러닝
   3. Spark Streaming : 스트리밍 데이터처리
   4. GraphX : 그래프 분석
5. Databricks
   1. Azure 기반
      1. delta : 데이터 전처리 손쉽게 사용, 아파치 스파크와 100% 연동
      2. Collaborative Notebook : 다양한 노트북 제공, 배치. 대쉬보드, BI툴 가능
      3. MLR : conda환경과 동일, 머신러닝 패키지 다수 설치
      4. Augmented 머신러닝 기능 제공  / 모델관리 : 수많은 experiment에 대한 추적과 비교분석, 기록관리, 최적 모델탐색 제공, 하이퍼파라미터 자동튜닝 



AWS, Azure, GCP 비교

- 머신러닝 온 클라우드 :  Azure 기능 많고, 드래그드랍가능
- STT TTS : Azure기능 많음
- 이미지분석 : GCP 기능 많음
- 비디오 : Azure 기능 많음, AWS만 스트리밍비디오 분석제공
